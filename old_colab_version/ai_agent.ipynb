{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuy10BJRWzmvmos39C7Qun"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1peOUt5DSgp"
      },
      "outputs": [],
      "source": [
        "#!pip install -U langgraph langsmith \"langchain[anthropic]\"\n",
        "!pip install -U langgraph langsmith langchain_openai langchain-google-genai\n",
        "!pip install -U datasets\n",
        "from PIL import Image\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gemini API**"
      ],
      "metadata": {
        "id": "uaPsmqbTM3Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "#_set_env(\"ANTHROPIC_API_KEY\")\n",
        "_set_env(\"GEMINI_API_KEY\")"
      ],
      "metadata": {
        "id": "944jPtCjDypo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Methods for LLM Communication**"
      ],
      "metadata": {
        "id": "HjVlMjDPM9Pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "import os\n",
        "import base64\n",
        "\n",
        "\n",
        "from io import BytesIO\n",
        "#from PIL import Image\n",
        "\n",
        "\n",
        "def encode_image(image_path):\n",
        "    \"\"\"Encodes an image to base64.\"\"\"\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "def encode_pil_image(pil_image):\n",
        "    \"\"\"Encodes an image to base64.\"\"\"\n",
        "    #return base64.b64encode(pil_image).decode(\"utf-8\")\n",
        "\n",
        "    #img = Image.open('test.jpg')\n",
        "    im_file = BytesIO()\n",
        "    pil_image.save(im_file, format=\"JPEG\")\n",
        "    im_bytes = im_file.getvalue()  # im_bytes: image in binary format.\n",
        "    im_b64 = base64.b64encode(im_bytes)\n",
        "    return im_b64.decode(\"utf-8\")\n",
        "\n",
        "def generate_pil_image_caption(model, pil_image, prompt=\"What is in this image?\"):\n",
        "    try:\n",
        "        # Encode the image to base64.\n",
        "        base64_image = encode_pil_image(pil_image)\n",
        "\n",
        "        # Prepare the message with the image and prompt.\n",
        "        message = HumanMessage(\n",
        "            content=[\n",
        "                {\"type\": \"text\", \"text\": prompt},  # Use the provided prompt\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
        "                },\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Generate the content.\n",
        "        response = model.invoke([message])\n",
        "        return response.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating caption: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_pil_image_diff_caption(model, pil_image_before, pil_image_after, prompt=\"What is in this image?\"):\n",
        "    try:\n",
        "        # Encode the image to base64.\n",
        "        base64_image_before = encode_pil_image(pil_image_before)\n",
        "        base64_image_after  = encode_pil_image(pil_image_after)\n",
        "\n",
        "        # Prepare the message with the image and prompt.\n",
        "        message = HumanMessage(\n",
        "            content=[\n",
        "                {\"type\": \"text\", \"text\": prompt},  # Use the provided prompt\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image_before}\"},\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image_after}\"},\n",
        "                },\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Generate the content.\n",
        "        response = model.invoke([message])\n",
        "        return response.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating caption: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_pil_sequence_diffs(model, pil_img_0, pil_img_1, pil_img_2, pil_img_3, pil_img_4, prompt=\"What is in this image sequence?\"):\n",
        "    try:\n",
        "        # Encode the image to base64.\n",
        "        base64_img_0 = encode_pil_image(pil_img_0)\n",
        "        base64_img_1 = encode_pil_image(pil_img_1)\n",
        "        base64_img_2 = encode_pil_image(pil_img_2)\n",
        "        base64_img_3 = encode_pil_image(pil_img_3)\n",
        "        base64_img_4 = encode_pil_image(pil_img_4)\n",
        "\n",
        "        # Prepare the message with the image and prompt.\n",
        "        message = HumanMessage(\n",
        "            content=[\n",
        "                {\"type\": \"text\", \"text\": prompt},  # Use the provided prompt\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img_0}\"},\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img_1}\"},\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img_2}\"},\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img_3}\"},\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img_4}\"},\n",
        "                },\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Generate the content.\n",
        "        response = model.invoke([message])\n",
        "        return response.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating caption: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def generate_pil_sequence_caption(model, pil_img_0, pil_img_1, pil_img_2, pil_img_3, pil_img_4, prompt=\"What is in this image sequence?\"):\n",
        "    try:\n",
        "        # Encode the image to base64.\n",
        "        base64_img_0 = encode_pil_image(pil_img_0)\n",
        "        base64_img_1 = encode_pil_image(pil_img_1)\n",
        "        base64_img_2 = encode_pil_image(pil_img_2)\n",
        "        base64_img_3 = encode_pil_image(pil_img_3)\n",
        "        base64_img_4 = encode_pil_image(pil_img_4)\n",
        "\n",
        "        # Prepare the message with the image and prompt.\n",
        "        message = HumanMessage(\n",
        "            content=[\n",
        "                {\"type\": \"text\", \"text\": prompt},  # Use the provided prompt\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img_0}\"},\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img_1}\"},\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img_2}\"},\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img_3}\"},\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_img_4}\"},\n",
        "                },\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Generate the content.\n",
        "        response = model.invoke([message])\n",
        "        return response.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating caption: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_image_caption(model, image_path, prompt=\"What is in this image?\"):\n",
        "    try:\n",
        "        # Encode the image to base64.\n",
        "        base64_image = encode_image(image_path)\n",
        "\n",
        "        # Prepare the message with the image and prompt.\n",
        "        message = HumanMessage(\n",
        "            content=[\n",
        "                {\"type\": \"text\", \"text\": prompt},  # Use the provided prompt\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
        "                },\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Generate the content.\n",
        "        response = model.invoke([message])\n",
        "        return response.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating caption: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_image_diff_caption(model, image_path_before, image_path_after, prompt=\"What is in this image?\"):\n",
        "    try:\n",
        "        # Encode the image to base64.\n",
        "        base64_image_before = encode_image(image_path_before)\n",
        "        base64_image_after  = encode_image(image_path_after)\n",
        "\n",
        "        # Prepare the message with the image and prompt.\n",
        "        message = HumanMessage(\n",
        "            content=[\n",
        "                {\"type\": \"text\", \"text\": prompt},  # Use the provided prompt\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image_before}\"},\n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image_after}\"},\n",
        "                },\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Generate the content.\n",
        "        response = model.invoke([message])\n",
        "        return response.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating caption: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_prolog_code(model, captions, prompt):\n",
        "    try:\n",
        "        # Prepare the message with the image and prompt.\n",
        "        message = HumanMessage(\n",
        "            content=[\n",
        "                {\"type\": \"text\", \"text\": captions + ' ' + prompt},\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Generate the content.\n",
        "        response = model.invoke([message])\n",
        "        return response.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating caption: {e}\")\n",
        "        return None\n",
        "\n",
        "def generate_answer_to_prompt(model, prompt):\n",
        "    try:\n",
        "        # Prepare the message with the image and prompt.\n",
        "        message = HumanMessage(\n",
        "            content=[\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Generate the content.\n",
        "        response = model.invoke([message])\n",
        "        return response.content\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating caption: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Read your API key from the environment variable or set it manually\n",
        "api_key = os.getenv(\"GEMINI_API_KEY\",\"xxx\")\n",
        "\n",
        "llm_model = ChatGoogleGenerativeAI(\n",
        "    model= \"gemini-2.0-flash\",\n",
        "    temperature=0.0,#1.0\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=1,#2,\n",
        "    google_api_key=api_key,\n",
        ")\n"
      ],
      "metadata": {
        "id": "_A3atNQmy_VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Google drive.**"
      ],
      "metadata": {
        "id": "FPuzkbSS91xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.listdir('/content/drive/MyDrive/content')\n",
        "\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('https://drive.google.com/drive/folders/15PpjM4NFRuCRSrnXWhvZwSm9spVBoTLm')\n",
        "#import os\n",
        "#os.listdir('/content/drive/MyDrive')\n",
        "\n",
        "#https://drive.google.com/drive/folders/15PpjM4NFRuCRSrnXWhvZwSm9spVBoTLm"
      ],
      "metadata": {
        "id": "SoOqajnap-bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Process samples into attached images**"
      ],
      "metadata": {
        "id": "VQSr0tEpe3S3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pictures = os.listdir('/content/drive/MyDrive/content/fire/sample_0')\n",
        "#print(pictures)\n",
        "import os\n",
        "from skimage import io\n",
        "#import cv2 as cv\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Convert FIRE samples\n",
        "#IMAGE_TYPE = 'fire'\n",
        "#IMAGE_TYPE = 'smoke'\n",
        "#IMAGE_TYPE = 'default'\n",
        "IMAGE_TYPE = '' # To not run sth by mistake\n",
        "\n",
        "the_path = '/content/drive/MyDrive/content/' + IMAGE_TYPE + '/'\n",
        "samples = os.listdir(the_path)\n",
        "samples.sort()\n",
        "total_samples = len(samples)\n",
        "print(samples)\n",
        "\n",
        "ii = 0\n",
        "for sample in samples:\n",
        "  sample_path = the_path + '/' + sample\n",
        "  imgs = os.listdir(sample_path)\n",
        "  imgs.sort()\n",
        "  print(sample, imgs)\n",
        "\n",
        "  img0_path = sample_path + '/' + imgs[0]\n",
        "  img1_path = sample_path + '/' + imgs[1]\n",
        "  img2_path = sample_path + '/' + imgs[2]\n",
        "  img3_path = sample_path + '/' + imgs[3]\n",
        "  img4_path = sample_path + '/' + imgs[4]\n",
        "\n",
        "  img0 = Image.open(img0_path)\n",
        "  img1 = Image.open(img1_path)\n",
        "  img2 = Image.open(img2_path)\n",
        "  img3 = Image.open(img3_path)\n",
        "  img4 = Image.open(img4_path)\n",
        "\n",
        "  #final_frame = cv.hconcat((img0, img1, img2, img3, img4))\n",
        "  #cv2_imshow(final_frame)\n",
        "  #cv2_imshow(new_img)\n",
        "\n",
        "  width, height = img0.size\n",
        "\n",
        "  total_width = 5*width\n",
        "\n",
        "  new_img = Image.new('RGB', (total_width, height))\n",
        "  new_img.paste(img0, (0*width,0))\n",
        "  new_img.paste(img1, (1*width,0))\n",
        "  new_img.paste(img2, (2*width,0))\n",
        "  new_img.paste(img3, (3*width,0))\n",
        "  new_img.paste(img4, (4*width,0))\n",
        "\n",
        "  total_width = int(total_width/2)\n",
        "  height = int(height/2)\n",
        "  new_img = new_img.resize((total_width, height))\n",
        "\n",
        "  #display(new_img)\n",
        "\n",
        "  if ii/total_samples < 0.75:\n",
        "    new_img.save('/content/drive/MyDrive/content/attached_imgs_dataset/train/'+IMAGE_TYPE+'/sample_'+str(ii)+'.jpg',quality=20)   #image.save(b.image.path,,optimize=True)\n",
        "  else:\n",
        "    new_img.save('/content/drive/MyDrive/content/attached_imgs_dataset/test/'+IMAGE_TYPE+'/sample_'+str(ii)+'.jpg',quality=20)\n",
        "    print('here')\n",
        "\n",
        "  ii = ii + 1\n"
      ],
      "metadata": {
        "id": "Pz4LyQVJe1bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading HuggingFace Dataset**"
      ],
      "metadata": {
        "id": "tn6u5QthLuxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "#dataset = load_dataset(\"imagefolder\", data_dir=\"/content/drive/MyDrive/content/attached_imgs_dataset\")\n",
        "train_ds, test_ds = load_dataset(\"imagefolder\", data_dir=\"/content/drive/MyDrive/content/attached_imgs_dataset/\", split=['train', 'test'])\n"
      ],
      "metadata": {
        "id": "cU7qFewYL0bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fire Logic with Dataset - FIRE**"
      ],
      "metadata": {
        "id": "fAy3E0FRVlCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for ii in [15,24]:#range(0, len(train_ds)):\n",
        "for ii in range(0, len(train_ds)):\n",
        "  curr_img = train_ds[ii]['image']\n",
        "  curr_lbl = train_ds[ii]['label']\n",
        "  #display(curr_img)\n",
        "  width, height = curr_img.size\n",
        "  new_width = int(width/5)\n",
        "  #print(width, height)\n",
        "\n",
        "  img_0 = Image.new('RGB', (new_width, height))\n",
        "  img_1 = Image.new('RGB', (new_width, height))\n",
        "  img_2 = Image.new('RGB', (new_width, height))\n",
        "  img_3 = Image.new('RGB', (new_width, height))\n",
        "  img_4 = Image.new('RGB', (new_width, height))\n",
        "\n",
        "  img_0.paste(curr_img.crop((0*new_width, 0, new_width, height)), (0,0))\n",
        "  img_1.paste(curr_img.crop((1*new_width, 0, 2*new_width, height)), (0,0))\n",
        "  img_2.paste(curr_img.crop((2*new_width, 0, 3*new_width, height)), (0,0))\n",
        "  img_3.paste(curr_img.crop((3*new_width, 0, 4*new_width, height)), (0,0))\n",
        "  img_4.paste(curr_img.crop((4*new_width, 0, 5*new_width, height)), (0,0))\n",
        "\n",
        "  #caption = generate_pil_image_caption(llm_model, img_0,\"What is in this image that is a fire emergency?\")\n",
        "  #print(curr_lbl, caption)\n",
        "  #diff1 = generate_pil_image_diff_caption(llm_model, img_0, img_1, \"What changes from the first image to the second that is a fire emergency?\")\n",
        "  #diff2 = generate_pil_image_diff_caption(llm_model, img_1, img_2, \"What changes from the second image to the second that is a fire emergency?\")\n",
        "  #diff3 = generate_pil_image_diff_caption(llm_model, img_2, img_3, \"What changes from the first image to the second that is a fire emergency?\")\n",
        "  #diff4 = generate_pil_image_diff_caption(llm_model, img_3, img_4, \"What changes from the first image to the second that is a fire emergency?\")\n",
        "  #print(diff1, diff2, diff3, diff4)\n",
        "  #prolog_input = caption + ' ' + diff1 + ' ' + diff2 + ' ' + diff3 + ' ' + diff4\n",
        "  #prolog_output = generate_prolog_code(llm_model, prolog_input, prompt=\"Generate a prolog program without comments based on the prompt that queries at the end whether there is a fire emergency.\")\n",
        "\n",
        "\n",
        "\n",
        "  #prolog_output = generate_pil_sequence_diffs(llm_model, img_0, img_1, img_2, img_3, img_4, \"Relevant to a fire emergengy, describe the first image, then describe the differences between the first and the second image, then the differences between the second and the third image, then the differences between the third and the fourth image, then the differences between the fourth and the fifth image. Generate a prolog program without comments based on the descriptions that queries at the end whether there is a fire emergency in the image sequence. Return only one yes or no.\")\n",
        "  prolog_output = generate_pil_sequence_diffs(llm_model, img_0, img_1, img_2, img_3, img_4, \"Relevant to a fire emergengy, describe what is going on. Then, based on the description, generate a prolog program without comments that queries whether there is a fire emergency in the image sequence. Return only one yes or no.\")\n",
        "\n",
        "\n",
        "  prolog_result = generate_prolog_code(llm_model, prolog_output, prompt=\"Run this prolog code and show me the result concisely without any reasoning, just yes or no.\")\n",
        "  #print(curr_lbl, '--------------------------------')\n",
        "  #print(prolog_output)\n",
        "  #print(prolog_result)\n",
        "  print(ii, curr_lbl, prolog_result)\n",
        "  print('================================')\n",
        "  time.sleep(30)"
      ],
      "metadata": {
        "id": "pGJng-pnVkBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparison code using LLM, non-Prolog logic**"
      ],
      "metadata": {
        "id": "9VtzoFdQFrv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ii in [15,24]:#range(0, len(train_ds)):\n",
        "#for ii in range(0, len(train_ds)):\n",
        "  curr_img = train_ds[ii]['image']\n",
        "  curr_lbl = train_ds[ii]['label']\n",
        "  display(curr_img)\n",
        "  width, height = curr_img.size\n",
        "  new_width = int(width/5)\n",
        "  #print(width, height)\n",
        "\n",
        "  img_0 = Image.new('RGB', (new_width, height))\n",
        "  img_1 = Image.new('RGB', (new_width, height))\n",
        "  img_2 = Image.new('RGB', (new_width, height))\n",
        "  img_3 = Image.new('RGB', (new_width, height))\n",
        "  img_4 = Image.new('RGB', (new_width, height))\n",
        "\n",
        "  img_0.paste(curr_img.crop((0*new_width, 0, new_width, height)), (0,0))\n",
        "  img_1.paste(curr_img.crop((1*new_width, 0, 2*new_width, height)), (0,0))\n",
        "  img_2.paste(curr_img.crop((2*new_width, 0, 3*new_width, height)), (0,0))\n",
        "  img_3.paste(curr_img.crop((3*new_width, 0, 4*new_width, height)), (0,0))\n",
        "  img_4.paste(curr_img.crop((4*new_width, 0, 5*new_width, height)), (0,0))\n",
        "\n",
        "  caption = generate_pil_sequence_caption(llm_model, img_0, img_1, img_2, img_3, img_4, \"Is there a fire emergency in this image sequence? Answer concisely without any reasoning, just yes or no. Aftter that, give a reasoning for the answer.\")\n",
        "  #print(curr_lbl, caption)\n",
        "\n",
        "  print(ii, curr_lbl, caption)\n",
        "  print('================================')\n",
        "  time.sleep(1)"
      ],
      "metadata": {
        "id": "DC3IqaZmGB0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prompting**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "LKaSaeKlVp89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer = generate_answer_to_prompt(llm_model, \"What prompt to use for you to detect fire in consecutive images?\")\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "ES_z6uEAV87x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trying out dspy.**"
      ],
      "metadata": {
        "id": "K5neZYcRWpYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U dspy"
      ],
      "metadata": {
        "id": "W8mMGJ2AWoYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dspy configuration.**"
      ],
      "metadata": {
        "id": "Tcw_sqnUXJbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "\n",
        "# Read your API key from the environment variable or set it manually\n",
        "api_key = os.getenv(\"GEMINI_API_KEY\",\"xxx\")\n",
        "\n",
        "llm_model = ChatGoogleGenerativeAI(\n",
        "    model= \"gemini-2.0-flash\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    google_api_key=api_key,\n",
        ")\n",
        "dspy.configure(lm=llm_model)\n"
      ],
      "metadata": {
        "id": "Avywa_vkXCQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dspy fire agent."
      ],
      "metadata": {
        "id": "N3QmxmOTc7_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = \"Is there an ongoing fire emergency?\"\n",
        "signature = dspy.Signature(\"claim -> titles: list[str]\", instructions)\n",
        "react = dspy.ReAct(signature, tools=[search_wikipedia, lookup_wikipedia], max_iters=20)\n",
        "\n",
        "# Create a huggingface dataset\n",
        "\n",
        "qa_pair = dspy.Example(question=\"What is in this image that is a burning fire emergency?\", answer=\"This is an answer.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "gzof51Kcc_Al"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}